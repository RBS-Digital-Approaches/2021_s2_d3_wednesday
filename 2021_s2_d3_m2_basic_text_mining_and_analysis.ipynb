{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2021_s1_d3_m2_basic_text_mining_and_analysis.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZRkKMCKNkvdi"},"source":["# I.  Basic Text Mining and Analysis:  Introduction"]},{"cell_type":"markdown","metadata":{"id":"RScxV--UlCxb"},"source":["Textual analysis is frequently referred to by a variety of names.  On the technical side, people often refer to Text Analysis, Text Mining, or Natural Language Processing.  Cultural heritage professionals and humanists also frequently refer to these methods as Distant Reading (appropriating a term that Franco Moretti, who used it to describe the processes of analyzing textual metadata) or Macroanalysis (a term derived form Matthew Jockers book of the same title which introduced various methods of textual analysis using the R language for statistical programming.)  \n","\n","In fact, the technical terms Text Analysis, Text Mining, and Natural Language Processing have slightly different meanings:\n","\n","\n","1.   **Text Mining** refers to process of computationally \"reading\" a text (or collection of texts) and extracting specific chunks of information.  It encapsulates process that convert unstructured text to structured data.\n","2.   **Text Analysis** refers to processes, such as basic descriptive statistical methods or more advanced machine learning methods, that analyze the information contained in texts.  Text Analysis is frequently performed on the structured data that results from Text Mining operations, but it can also be performed directly on unstructured texts.  Text Analysis processes typically return summary data, such as lists of all references to particular dates, persons, and topics or summary statistics regarding word or phrase usage and frequency.  \n","3.   **Natural Language Processing** (NLP) describes a particular subset of Text Mining and Text Analysis processes that utilize the grammatical and semantic structures associated with natural languages as part of the mining and analysis process.  Processes that do not account for the naturalness of language, such as word frequency analysis, cannot properly be consider Natural Language Processing.\n","\n","This self-study module introduces basic skills and methods of text mining and analysis, focusing most but not exclusively on analytic methods rooted in descriptive statistical analysis.  As the name implies, Descriptive Statistical methods describe the basic features of the data being examined, providing simple but powerful summaries about the data.\n"]},{"cell_type":"markdown","metadata":{"id":"VrayDq7yqKOr"},"source":["# II.  Python Packages for Text Mining and Analysis"]},{"cell_type":"markdown","metadata":{"id":"gLc9k4lCqYMn"},"source":["A wide variety of Python packages and modules are available for performing text mining and analysis.  When executed, the code cell below will load those necessary to perform the activities presented in this course module.  Comments in the code identify each of the packages being loaded.  In each case, you can refer to the package documentation for more specific information about the package being used.  You must run the code cells below to properly prepare your environment to perform the text mining and analysis tasks presented in this module."]},{"cell_type":"code","metadata":{"id":"ZGDtzniRIEix"},"source":["# update collab environment to latest version of NLTK\n","# documentation: https://www.nltk.org/\n","!pip install nltk -U"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zvMcLYrzWZ67"},"source":["# import the base nltk package\n","import nltk\n","\n","# import the nltk text module\n","from nltk import text\n","\n","# import the nltk sentence tokenizer\n","from nltk.tokenize import sent_tokenize\n","\n","# import the nltk word_tokenize module\n","from nltk import word_tokenize\n","\n","# import the nltk corpus reader module\n","from nltk.corpus import PlaintextCorpusReader\n","\n","# import the nltk probability module\n","from nltk import probability\n","\n","# import the nltk bigrams module\n","from nltk import bigrams\n","\n","# import the nltk tag.pos_tag module\n","from nltk.tag import pos_tag\n","\n","# import nltk.chunk modules \n","from nltk.chunk import conlltags2tree, tree2conlltags\n","\n","# import the nltk POS tagger\n","nltk.download('averaged_perceptron_tagger')\n","\n","# import th nltk dispersion_plot module\n","from nltk.draw.dispersion import dispersion_plot\n","\n","# download the nltk named entity chunker\n","nltk.download('maxent_ne_chunker')\n","\n","# download the ntltk words library (for chunking)\n","nltk.download('words')\n","\n","# import the punkt sentence parser for nltk\n","# documentation: https://www.kite.com/python/docs/nltk.punkt\n","nltk.download('punkt')\n","\n","# import the pprint package\n","# documentation: https://docs.python.org/3/library/pprint.html\n","from pprint import pprint\n","\n","# import collections python module\n","# documentation: https://docs.python.org/3/library/collections.html\n","import collections\n","\n","# import the collections package counter module\n","from collections import Counter\n","\n","# import the pandas package\n","# documentation: https://pandas.pydata.org/docs/\n","import pandas\n","\n","# import the networkx package\n","# documentation: https://networkx.org/documentation/stable/index.html\n","import networkx\n","\n","# import the python regular expression package\n","# documentation: https://docs.python.org/3/library/re.html\n","import re\n","\n","# import Spacy NLP package \n","# documentation: https://spacy.io/\n","import spacy\n","\n","# import the Spacy display module\n","from spacy import displacy\n","\n","# import the english core web sm english natural language module\n","# documentation: https://spacy.io/usage/models\n","import en_core_web_sm\n","\n","# import the matplotlib package\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qItWOMNKtu2S"},"source":["# III.  Load a Working File"]},{"cell_type":"markdown","metadata":{"id":"PSf7SBqHt4nl"},"source":["There are various ways of loading files for text mining and analysis.  Many text mining packages perform multiple modes of initial processing on files as they are being loaded.  In this module, we will directly load the contents of single file as our working file without performing any pre or load-time processing.  In future modules we will introduce other methods of loading collections of text files as a \"corpus\" for analysis."]},{"cell_type":"markdown","metadata":{"id":"LeXFdNBIu011"},"source":["Before you can load a file for analysis, you must mount your Google Drive in this environment."]},{"cell_type":"code","metadata":{"id":"5m5fdnh2vAQW"},"source":["from google.colab import drive\n","drive.mount('/gdrive/')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2MGEZCbfvMsS"},"source":["Once your Google Drive has successfully mounted, you can open one of the sample data files provided for the course or a file of your own that you have placed in the “data_my” directory of the Course Home Directory:\n","\n","1.   To load a course sample file, you can simply run the code cell below.\n","2.   To load a text (ASCII) file of your own, place a hashtag in from of the line that points to the file at \"/gdrive/MyDrive/rbs_digital_approaches_2021/data_class/melville.txt\",\n","replace the \"\\<name_of_your_file.txt\\>” substring in the line that reads, \" /gdrive/MyDrive/rbs_digital_approaches_2021/data_my/\\<name_of_your_file.txt\\>\" with the name of your file, uncomment the line, and then run the cell.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"enKrUpF4wZwm"},"source":["working_file_path = \"/gdrive/MyDrive/rbs_digital_approaches_2021/data_class/melville.txt\"\n","#working_file_path = \"/gdrive/MyDrive/rbs_digital_approaches_2021/data_my/<name_of_your_file.txt>\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HWwLexukweVo"},"source":["Now that you've defined a file to load, we can open the file and read its contents into a string variable."]},{"cell_type":"code","metadata":{"id":"VNRlMPS1Wzi3"},"source":["# open a text file for processing\n","working_file = open(working_file_path, \"r\")\n","\n","# read the file contents into a string variable\n","working_text = working_file.read()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WHlWPWmZxlNC"},"source":["You can check that your file loaded by checking the length and examining the opening characters of the working_text variable."]},{"cell_type":"code","metadata":{"id":"pzNkffHdx3us"},"source":["# print the character length of our working text\n","print('Characters in string:', len(working_text))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5_SZpjdR27mg"},"source":["# look at the first 200 characters of the string\n","print(working_text[0:200:1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k7uZRUI9zLHo"},"source":["# IV. Preliminary Analysis (Data Forensics)"]},{"cell_type":"markdown","metadata":{"id":"dmK3ik6SzSmR"},"source":["Whenever you begin a new text or data analysis process, the first thing that you should do is perform some preliminary analysis.  This crucial first step, known as Data Forensics, is frequently overlooked, but it is crucial to helping us understand that actual state of our data and, more importantly, the extent and nature of cleaning and preparation that we need to do in order to ensure that our planned mining and analysis return valid results.  \n","\n","The remainder of this workbook is dedicated to performing various modes of preliminary analysis.  As you perform these forensics, take note of any anomalies you see in the textual data.  The point of the preliminary analysis is to reveal the types and extent of text preparation that will have to be performed prior to analysis.  Are there parts of the text that you would want to remove before analysis?  Obvious errors in OCR or transcription that might need to be cleaned?  Things that are being mis-interpreted by the computer that might affect future analysis? All of this is crucial information.  As you work through the notebook, there are many prompts to look at portions of the data.  Don’t be afraid to change the parameters of these prompts and gain other views of the data.  \n","\n","Exploration and discovery are the purpose of this exercise.  Our group activity in the next class meeting will be to develop and implement a cleaning strategy based on what you and your classmates find and document in this activity. So take good notes and be prepared to share them at our next meeting.\n"]},{"cell_type":"markdown","metadata":{"id":"ujo2LW0c_Sik"},"source":["# V.  Chunking and Tokenizing"]},{"cell_type":"markdown","metadata":{"id":"Ugg7bKiZ0KTg"},"source":["First, let's do some chunking.  **Chunking** (yes, that's the real, technical term) is a process of breaking a text into constituent parts, such as paragraphs, sentences, or phrases.  Here, we'll chunk into sentences."]},{"cell_type":"markdown","metadata":{"id":"XJJXdM6-1Ju5"},"source":["Note that while this is primarly a module on text minging and analysis, sentnce chunking is actually a Natural Language Processing operation.  Here, the sent_tokenize() function relies on the english language model that we loaded during our environment setup to apply rules for sentence formation and representation in the lanaguage (natural language information) to chunk the text into a list of sentences."]},{"cell_type":"code","metadata":{"id":"XpQS1WARJu7E"},"source":["# tokenize the text by sentence\n","sentence_list = sent_tokenize(working_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yl-EgWWQ08Ru"},"source":["Before we procede, let's look at our sentence tokens to make sure the process worked."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mfHnIx7I16yA","executionInfo":{"status":"ok","timestamp":1626182304990,"user_tz":420,"elapsed":115,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}},"outputId":"5fbeaf77-9969-4d41-a469-69c1c7b736b1"},"source":["# print the length of the sentence_list list\n","print('Sentences in text:', len(sentence_list))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sentences in text: 10099\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MD8z31K92Pvy"},"source":["# look at the first ten sentences\n","print(sentence_list[0:10:1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TuBuIDf-3qU9"},"source":["Now let's also **tokenize** on individual words. Like chunking, tokenizing is a prcess of splitting the text into a list of consitutent parts, in this case, words.  Since words are a minimal semantic unit, we call this tonization rather than chunking."]},{"cell_type":"code","metadata":{"id":"GGbBHzCTaMXB"},"source":["# tokenize the text by word\n","word_tokens = word_tokenize(working_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"95o2C1dP6UAC"},"source":["And, again, we'll examine the results."]},{"cell_type":"code","metadata":{"id":"gfSKFvtD6Yd9"},"source":["# print the length of the word_tokens list\n","print('Words in text:', len(word_tokens))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W3NlBYmi6ZB3"},"source":["# look at an arbitrary selection of words\n","print(word_tokens[500:1000:10])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qqSURsk8_cdf"},"source":["# VI.  Frequency Distributions"]},{"cell_type":"markdown","metadata":{"id":"F5x1icWP7RvT"},"source":["Now that we have out text chunked and tokenized, we can analyze some frequency distributions of words across the text."]},{"cell_type":"code","metadata":{"id":"GtIe_Dx4eToB"},"source":["# create a frequency distribution using NLTK\n","freq_dist = nltk.probability.FreqDist(word_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pKFxI2ao7m3b"},"source":["# Look at the most frequent words\n","print(freq_dist.most_common(n=100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ejhWKDPaBpy0"},"source":["We can also plot our frequency distribution:"]},{"cell_type":"code","metadata":{"id":"7FBXV6ozodT5"},"source":["# set the size of the plot\n","plt.figure(figsize=(12, 9))\n","# plot the freqiuency distribution of the top words\n","freq_dist.plot(50, cumulative=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kqw_MvfL8Q6s"},"source":["Now that we've dived more deeply into the words in our text, we might want to also take some time to examine particular words of interest by retrieving the count for our word of interest."]},{"cell_type":"code","metadata":{"id":"xgaDgMLM2upU"},"source":["# count the occurrences of a word of interest\n","word_tokens.count('whale')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KnX_SO1Y_5a0"},"source":["# VII.  Key Word in Context (KWIC)"]},{"cell_type":"markdown","metadata":{"id":"S0PdwMSH88DY"},"source":["Now that we've dived more deeply into the words in our text, we might want to also It can also be useful to examine the context in which particular words appear.  For example, we might have prior knowledge about the importance of a word of interest, or we might have seen something earlier in our analysis that prompts us to want to look deeper into a particular word.  To accomplish this, we first create a concordance for the text.  A concordance is an index that tracks the location in the text of every occurrence of every word.\n","\n"]},{"cell_type":"code","metadata":{"id":"HiO975PNYtd1"},"source":["# create a condorance obj\n","obj_concord = nltk.text.ConcordanceIndex(word_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uH7VN0kC9YR7"},"source":["Once we have a concordance, we can query it for a word of interest and return a designated number of characters on either side of each of a designated number of occurrences."]},{"cell_type":"code","metadata":{"id":"oawFVrbP77nM"},"source":["obj_concord.print_concordance(\"whale\", width=80, lines=25)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ub1UsqNpsS1Q"},"source":["# VIII.  Word Cooccurrence Networks (n-grams)"]},{"cell_type":"markdown","metadata":{"id":"LPO7Jyd5Ee9I"},"source":["Let's also take some time to do some preliminary analysis of which words tend to cooccur in the text.  For our preliminary analysis, we'll look only at bigrams, which are pairs of words that frequently appear next to each other in the text."]},{"cell_type":"code","metadata":{"id":"I9PtVCzdqKVG"},"source":["# create a list of bigrams\n","bigram_list = list(bigrams(word_tokens))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L4K8uzcS6kcB"},"source":["# create a count of unique bigrams\n","bigram_counts = collections.Counter(bigram_list)\n","print(bigram_counts)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zWj4erYdFHEi"},"source":["Now that we have a frequency tagged list of bigrams that appear in the text, we can work on plotting a network graph to represent the top bigrams.  We'll do this using the Pandas and Networkx packages.\n","\n","The Networks package, which we will use to draw the network graph, expects to receive data in the form a data.frame (a data.frame is spreadsheet-like data structure that contains columns, each of which is a field (or a variable in statistical language) and rows (each of which represents a single item, or observation in statistical language).  \n","\n","At present, our bigram data is in the form of a list of key/value pairs.  Happily, the Pandas package has functions for creating and working with data.frames, so we'll use Pandas to convert the data into a data.frame and then send that data.frame to Networkx to draw our plot.\n"]},{"cell_type":"code","metadata":{"id":"S2_I2Ecc7mKL"},"source":["# create an empty pandas DataFrame\n","bigram_df = pandas.DataFrame(data=None, columns=['source', 'target', 'weight'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iciXQldtGX-T"},"source":["# add top bigram items to the dataframe\n","for x, z in bigram_counts.most_common(20):\n","  bigram_df.loc[len(bigram_df.index)] = [x[0], x[1], z] "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MrQmd3s9GaPU"},"source":["# create the nodegraph using Networkx\n","net_graph = networkx.from_pandas_edgelist(bigram_df, source='source', target='target', edge_attr='weight')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qn3KpT8HGjgP"},"source":["And now we're ready to draw the plot to screen."]},{"cell_type":"code","metadata":{"id":"97TlvM6dI6Wu"},"source":["# set the size of the plot\n","plt.figure(figsize=(12, 9))\n","# draw the graph as a force directed graph\n","networkx.draw_networkx(net_graph, with_labels=True, font_size=24)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LSq76lYNGvq9"},"source":["We can also draw the network as a circular rather than force directed graph."]},{"cell_type":"code","metadata":{"id":"dhtck98DJAZz"},"source":["# set the size of the plot\n","plt.figure(figsize=(12, 9))\n","# draw the graph as a circular graph\n","networkx.draw_circular(net_graph, with_labels=True, font_size=24)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ANAtbs3CudT"},"source":["# IX. Word Occurrence Dispersion Plot"]},{"cell_type":"markdown","metadata":{"id":"PpDRdxu2C0j1"},"source":["We can also plot the dispersion word occurrence across the narrative time of the text.  This allows us to see how different words function in the text."]},{"cell_type":"code","metadata":{"id":"0TWYhN4HDGfj"},"source":["# set the size of the plot\n","plt.figure(figsize=(12, 9))\n","# define the words you want to plot\n","targets=['whale', 'ahab', 'ship', 'light', 'water', 'I']\n","#draw the plot\n","dispersion_plot(word_tokens, targets, ignore_case=True, title='Lexical Dispersion Plot')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9QZsypT0Cmb9"},"source":["# X. Part of Speech (POS) Analysis"]},{"cell_type":"markdown","metadata":{"id":"IZ1z0W2eHEgr"},"source":["We can also do some simple analysis of the distribution of parts of speech in the text.  Note that this is solidly in the camp of Natural Language Processing, since NLTK uses English natural language models to parse the text into sentences, phrases, and finally POS based on the grammatical structure of the English language."]},{"cell_type":"code","metadata":{"id":"X3WEwOVM2d5C"},"source":["# First we'll perfor the pos tagging\n","tagged = nltk.pos_tag(word_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yR54-HOMITqG"},"source":["Now that we've tagged the text for POS, let's take a quick look at the result."]},{"cell_type":"code","metadata":{"id":"vHw1UEn1I30d"},"source":["# look at the first ten POS tagged words\n","print(tagged[0:10:1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gHERbcHuSB1L"},"source":["<font size=\"2\">note:  a key to the POS tags applied by the NLTK can be found in the appendix at the end of this notebook."]},{"cell_type":"markdown","metadata":{"id":"qyekZmClJD6k"},"source":["We see in the above that the results of our POS tagging are returned as a list of key/value pairs where the 'key' is the word and the 'value' is the code for the part of speech that the computer has determined for each key.  \n","\n","Next, we'll extract just the POS tags from the pairs and save them as a list."]},{"cell_type":"code","metadata":{"id":"gkP3L_QQCsDH"},"source":["# create a list of just POS tags\n","pos_list = []\n","for word, pos in tagged:\n","  pos_list.append(pos)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TElvHEUyJwjK"},"source":["Now, let's look at this list to make sure we got it right."]},{"cell_type":"code","metadata":{"id":"trnLRiXiJ09O"},"source":["# first, see how long the list is.  It should be the same length as our \n","# original list of words in the text, which we've already calculated above.\n","print(len(pos_list))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Dgride0KIWy"},"source":["# Now we'll look at the first ten POS tags in this list to see if\n","# everything looks right\n","print(pos_list[0:10:1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HRiz3RR9KbHx"},"source":["Now, we'll create a frequency distribution of our POS tags and examine the distribution."]},{"cell_type":"code","metadata":{"id":"a4xLKGw0C4X1"},"source":["# create a frequency distribution of POS\n","pos_freq_dist = nltk.probability.FreqDist(pos_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iZXybmzqC-gx"},"source":["# Look at the most frequent POS\n","print(pos_freq_dist.most_common(n=50))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MMeluUCfKmjn"},"source":["And, finally, we'll plot the POS frequncy distribution."]},{"cell_type":"code","metadata":{"id":"Eql6WGkwDiTk"},"source":["# set the size of the plot\n","plt.figure(figsize=(12, 9))\n","# plot the freqiuency distribution of POS\n","pos_freq_dist.plot(50, cumulative=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V3d7MNaKE9dY"},"source":["We can also do a dispersion plot of parts of speech to see if there are patterns that represent stylistic shifts across the time of the novel."]},{"cell_type":"code","metadata":{"id":"WnujqwNmFK79"},"source":["# set the size of the plot\n","plt.figure(figsize=(12, 9))\n","# define the words you want to plot\n","targets=['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS', 'NNP', 'NNPS', 'PRP', 'PRP$']\n","#draw the plot\n","dispersion_plot(pos_list, targets, ignore_case=True, title='POS Dispersion Plot')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eS-bXOYRK42d"},"source":["# XI.  Wrapping Up"]},{"cell_type":"markdown","metadata":{"id":"M6ocV4XGK9uz"},"source":["At this point, you've examined the text from a variety of perspectives:  As a text blob (the full text that you originally loaded into the system); as sentences; as words, including some preliminary analysis of the relationships between words, and you've delved a bit into the grammatical structure of the text by looking at parts of speech.  \n","\n","As per the introductory text to section IV, you should generally make a habit of taking detailed notes of any trends and/or issues you observed in the text while performing your preliminary analysis.  (Hopefully you did this in this instance.) Performing an initial forensic examination of your data and documenting it well is one of the most important, and frequently overlooked, steps in the data/text analysis pipeline.  The information that you gain during this forensics is crucial to making good decisions about what needs to be done to prepare a text for analysis to answer specific scholarly questions about the text.\n","\n","When we meet in our next discussion session, we’ll discuss everyone’s findings and co-code some text cleaning and processing."]},{"cell_type":"markdown","metadata":{"id":"oZLAPi5FScwN"},"source":["# Appendix:  NLTK POS Tag Key"]},{"cell_type":"markdown","metadata":{"id":"UPJVtFjhSjFA"},"source":["The following is a key to the POS tags applied by the NLTK when performing POS tagging."]},{"cell_type":"markdown","metadata":{"id":"YaEd9bGjSqc2"},"source":[" \n","*   CC   | coordinating conjunction\n","*   CD   | cardinal digit\n","*   DT   | determiner\n","*   EX   | existential there (ex: 'there is')\n","*   FW   | foreign word\n","*   IN   | preposition/subordinating conjunction\n","*   JJ   | adjective (ex: big)\n","*   JJR  | adjective, comparative (ex: bigger)\n","*   JJS  | adjective, superlative (ex: biggest)\n","*   LS   | list marker (ex: '1)'\n","*   MD   | modal (ex: could, will)\n","*   NN   | noun, singular\n","*   NNS  | noun plural\n","*   NNP  | proper noun, singular\n","*   NNPS | proper noun, plural\n","*   PDT  | predeterminer (ex: 'all the kids')\n","*   POS  | possessive ending (ex: Sam's)\n","*   PRP  | personal pronoun\n","*   PRP\\$ | possessive pronoun \n","*   RB   | adverb (ex: very) \n","*   RBR  | adverb, comparative (ex: better)\n","*   RBS  | adverb, superlative (ex: best)\n","*   RP   | particle \n","*   TO   | to (ex: to go 'to' the store.)\n","*   UH   | interjection \n","*   VB   | verb, base form (ex: take)\n","*   VBD  | verb, past tense (ex: took)\n","*   VBG  | verb, gerund/present participle (ex: taking)\n","*   VBN  | verb, past participle (ex: taken)\n","*   VBP  | verb, sing. present, non-3d (ex: take)\n","*   VBZ  | verb, 3rd person sing. present (ex: takes)\n","*   WDT  | wh-determiner (ex: which)\n","*   WP   | wh-pronoun (ex: who, what)\n","*   WP\\$  | possessive wh-pronoun (ex: whose)\n","*   WRB  | wh-abverb (ex: where, when)"]}]}