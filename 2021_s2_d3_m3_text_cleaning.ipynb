{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2021_s2_d3_m3_text_cleaning.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZRkKMCKNkvdi"},"source":["# I.  Text Cleaning and Preparation:  Introduction"]},{"cell_type":"markdown","metadata":{"id":"RScxV--UlCxb"},"source":["This workbook presents some basic code for text cleaning and preparation.  While some of the code looks for text patterns specific to the Project Gutenberg EBook used as an exanple, the process presented is generic and applicable to all text cleaning and preparation workflows."]},{"cell_type":"markdown","metadata":{"id":"VrayDq7yqKOr"},"source":["# II.  Setup the Environment"]},{"cell_type":"markdown","metadata":{"id":"gLc9k4lCqYMn"},"source":["We'll focus on usingthe NLTK package to clean and process our text for final analysis.  Comments in the code identify each of the packages being loaded.  In each case, you can refer to the package documentation for more specific information about the package being used.  You must run the code cells below to properly prepare your environment to perfrom the text mining and analysis tasks presented in this module."]},{"cell_type":"code","metadata":{"id":"ZGDtzniRIEix"},"source":["# update collab environment to latest version of NLTK\n","# documentation: https://www.nltk.org/\n","!pip install nltk -U"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a612AzoBqW6B"},"source":["# update to the latest version fo Spacy\n","# https://spacy.io/\n","!pip install spacy -U"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zvMcLYrzWZ67"},"source":["# import the base nltk package\n","# and required modules\n","# https://www.nltk.org/\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","\n","# download nltk language models\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# import the python regular expression package\n","# https://docs.python.org/3/library/re.html\n","import re\n","\n","# inport the string package\n","# https://docs.python.org/3/library/string.html\n","import string\n","\n","# import Spacy NLP package\n","# https://spacy.io/\n","import spacy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WkUWCW8IrbSZ"},"source":["# download and install the spacy language model\n","!python -m spacy download en_core_web_sm\n","sp=spacy.load('en_core_web_sm')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qItWOMNKtu2S"},"source":["# III.  Load the Text File"]},{"cell_type":"code","metadata":{"id":"5m5fdnh2vAQW"},"source":["from google.colab import drive\n","drive.mount('/gdrive/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"enKrUpF4wZwm","executionInfo":{"status":"ok","timestamp":1626821802673,"user_tz":420,"elapsed":260,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["working_file_path = \"/gdrive/MyDrive/rbs_digital_approaches_2021/s2_data_class/melville.txt\""],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HWwLexukweVo"},"source":["Now that you've defined a file to load, we can open the file and read its contents into a string variable."]},{"cell_type":"code","metadata":{"id":"VNRlMPS1Wzi3","executionInfo":{"status":"ok","timestamp":1626821809238,"user_tz":420,"elapsed":2567,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# open a text file for processing\n","working_file = open(working_file_path, \"r\")\n","\n","# read the file contents into a string variable\n","working_text = working_file.read()"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WHlWPWmZxlNC"},"source":["You can check that your file loaded by checking the length and examining the opening characters of the working_text variable."]},{"cell_type":"code","metadata":{"id":"pzNkffHdx3us"},"source":["# print the character length of our working text\n","# and the first several characters\n","print('Characters in string:', len(working_text))\n","print(working_text[:600:1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gVK3DYKDJF42"},"source":["Now let's print a \"representation\" version of the string, which shows all hiddent characters:"]},{"cell_type":"code","metadata":{"id":"0DXW5fkpJNzV"},"source":["print(repr(working_text[:600:1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cuBJlhl4mgBh"},"source":["IV.  Convert to Lowercase"]},{"cell_type":"markdown","metadata":{"id":"yS3b7yZsl9WN"},"source":["# IV.  Remove Newline Characters and Strip Spaces"]},{"cell_type":"code","metadata":{"id":"T9bwg0lcmFli","executionInfo":{"status":"ok","timestamp":1626821999223,"user_tz":420,"elapsed":264,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# define a pattern for finding newlines\n","pattern = re.compile(r\"\\n\", re.DOTALL | re.MULTILINE | re.IGNORECASE)\n","# run the replacement.  \n","working_text = re.sub(pattern, \" \", working_text)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"gYW_tB6HsCfI","executionInfo":{"status":"ok","timestamp":1626822027982,"user_tz":420,"elapsed":716,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# define a patern for finding multiple spaces\n","pattern = re.compile(r\"\\s+\")\n","# run the replacement\n","working_text = re.sub(pattern, \" \", working_text)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"-stuZMLxrfp_","executionInfo":{"status":"ok","timestamp":1626822031733,"user_tz":420,"elapsed":267,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# strip leading and trailing spaces\n","working_text = working_text.strip()"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BPvmNdoQJkGG"},"source":["Now let's look at the state of the text."]},{"cell_type":"code","metadata":{"id":"Q8pheCbkJof3"},"source":["print(working_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y51Y1tFaMkZ1"},"source":["# IV. Remove Paratext"]},{"cell_type":"code","metadata":{"id":"LJpSnomdQgrd","executionInfo":{"status":"ok","timestamp":1626822139886,"user_tz":420,"elapsed":272,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# define a pattern and remove the opening text\n","pattern = re.compile(r\"^.*chapter 1\\. Loomings\\.?\", re.IGNORECASE)\n","working_text = re.sub(pattern, \"\", working_text)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"8qn5hHMgz2wm","executionInfo":{"status":"ok","timestamp":1626822143469,"user_tz":420,"elapsed":282,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# define a pattern and remove the closing text\n","pattern = re.compile(r\"End of Project Gutenberg's.*\", re.IGNORECASE)\n","working_text = re.sub(pattern, \"\", working_text)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"oTmux5QznszI"},"source":["# look at the first 50 characters of the string\n","print(working_text[0:50:1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UrFm6djbKF_Z"},"source":["# look at the last 50 characters of the string\n","print(working_text[1189996:1190046:1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3s8Jk73t16X0"},"source":["# V. Save Clean Blob Version"]},{"cell_type":"markdown","metadata":{"id":"Kjirbci9QVRX"},"source":["It's a good idea to put aside a version of the minimally claeaned text as single blob for use later. "]},{"cell_type":"code","metadata":{"id":"X4x4G0Nx2DFJ","executionInfo":{"status":"ok","timestamp":1626822332162,"user_tz":420,"elapsed":274,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["blob_text = working_text"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2JZkHW0l5zws"},"source":["# For the rest of this stage of cleaning we'll tokenize and then clean because NLTK Likes it that way"]},{"cell_type":"code","metadata":{"id":"7_vKy4c95-9H","executionInfo":{"status":"ok","timestamp":1626822343085,"user_tz":420,"elapsed":1907,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# tokenize on words\n","tokens = word_tokenize(working_text)\n"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"HTMuWbEB8ip_"},"source":["# look at the results\n","print(tokens[:10])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y9jfxpoG8PD-","executionInfo":{"status":"ok","timestamp":1626822385505,"user_tz":420,"elapsed":284,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# remove punctuation from each word\n","table = str.maketrans('', '', string.punctuation)\n","filtered_tokens = [w.translate(table) for w in tokens]"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"mg0CPx7E9IVS"},"source":["# look at the results\n","print(filtered_tokens[:10])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1-bJFaPp9IHP","executionInfo":{"status":"ok","timestamp":1626822430894,"user_tz":420,"elapsed":281,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# remove remaining tokens that are not alphanumeric\n","filtered_tokens = [word for word in filtered_tokens if word.isalpha()]"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Cy8M7Gr90Ak"},"source":["# look at the results\n","print(filtered_tokens[:50])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZkEzXADI9zZq","executionInfo":{"status":"ok","timestamp":1626822471447,"user_tz":420,"elapsed":268,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# load the nltp stopword list\n","stop_words = set(stopwords.words('english'))\n"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"eXjxqA-B-xI4"},"source":["# review the stopwords\n","print(stop_words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cWa4zTlfB0c8"},"source":["Note that based on your research qustion you might want to modify the stopword list.  You can reuse code from above (for removing spaces, etc.) to create a list of words you want to remoove from the stopword list.  Alternatively, you can append other words to this list or build your own from scratch."]},{"cell_type":"code","metadata":{"id":"hlBnhX4A-zUc","executionInfo":{"status":"ok","timestamp":1626822501546,"user_tz":420,"elapsed":261,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# remove the stopwords\n","filtered_tokens = [w for w in filtered_tokens if not w in stop_words]"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"KZMbsYD6Ay6i"},"source":["# look at the results\n","print(filtered_tokens[:50])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e6gU6fkWBnY1"},"source":["# Stemming\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GMynaab7RGbe"},"source":["Stemming uses a rules-based algorithm to remove plural endings, \"ing\" endings and the like from words as a means of reducing variation.  [See the Wikipedia article here for a good overview](https://en.wikipedia.org/wiki/Stemming)."]},{"cell_type":"code","metadata":{"id":"_mWtdD8hCKp3","executionInfo":{"status":"ok","timestamp":1626822547072,"user_tz":420,"elapsed":1883,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# instantiate a porter stemmer class object\n","p_stemmer = PorterStemmer()\n","\n","#  run the stemmer on our list of filtered words\n","stemmed_tokens = [p_stemmer.stem(word) for word in filtered_tokens]"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"cXLeUMBPC9P8"},"source":["# view the results\n","print(stemmed_tokens[:100])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qnf8mMY9q390"},"source":["# Lemmatizing"]},{"cell_type":"markdown","metadata":{"id":"pFsMju-DRpVS"},"source":["Lemmatization is an NLP based reduction method that uses language models to reduce all variants of word (is, are, am, etc.) to a common linguistic root (be).  It generally improves the quality of semantic models because it reduces lexical variation in favor of semantic sameness.  However, in many cases we care a lot about particular words, and stemming often erases these differences, so think carefully before stemming.  Also note that the stemming algorithm operates on a fulltext blob, not on a tokenized list of words.  (Remember, we saved this above for future use in a variable names blob_text.)"]},{"cell_type":"code","metadata":{"id":"cpx6hXiyq89x"},"source":["sp_text = sp(blob_text[:100:1])\n","for word in sp_text:\n","  print(word.text, word.lemma_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KnPZRzlgO040"},"source":["Before we can actually lemmatize an entire text, we have to exapand the max size of the memory allocation devoted to the Spacy language model to handle a text of this length."]},{"cell_type":"code","metadata":{"id":"wE5CZB0sM-cL"},"source":["# get the character length of the text blob\n","len(blob_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7XtC1couNCCn","executionInfo":{"status":"ok","timestamp":1626823154722,"user_tz":420,"elapsed":263,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# set the max character length of the spacy object\n","sp.max_length = 1190050"],"execution_count":44,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O-fkC5OhPM4B"},"source":["Here we run the code to preform the lemmatization.  Note that this will take several minutes to run."]},{"cell_type":"code","metadata":{"id":"ZnCcJE_MMel6","executionInfo":{"status":"ok","timestamp":1626824026970,"user_tz":420,"elapsed":199754,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["lemma_tokens = [word.lemma_ for word in sp(blob_text)]"],"execution_count":47,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F-mGDCaOPT-_"},"source":["Now look at the output."]},{"cell_type":"code","metadata":{"id":"BYOtfPn5Oqxe"},"source":["print(lemma_tokens[:50])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dH744ijAPaEH"},"source":["Now, we need to remove all spaces, non alphanumerics, etc. from our lemmatized list to get to clean text.  Note that this is the exact same process we ran our original list of word tokens above."]},{"cell_type":"code","metadata":{"id":"4cd_l-MyPgUb","executionInfo":{"status":"ok","timestamp":1626824093377,"user_tz":420,"elapsed":289,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# remove punctuation from the list\n","table2 = str.maketrans('', '', string.punctuation)\n","filtered_lem_tokens = [w.translate(table2) for w in lemma_tokens]\n","\n","# remove remaining tokens that are not alphanumeric\n","filtered_lem_tokens = [word for word in filtered_lem_tokens if word.isalpha()]\n","\n","# remove the stopwords\n","filtered_lem_tokens = [w for w in filtered_lem_tokens if not w in stop_words]"],"execution_count":49,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8trz5WIbRcCf"},"source":["Now let's look at our lemmatized token list.\n","\n"]},{"cell_type":"code","metadata":{"id":"qVismW-wRbd8"},"source":["print(filtered_lem_tokens[0:50:1])"],"execution_count":null,"outputs":[]}]}